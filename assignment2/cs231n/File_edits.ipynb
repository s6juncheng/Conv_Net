{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class FullyConnectedNet(object):\n",
    "  \"\"\"\n",
    "  A fully-connected neural network with an arbitrary number of hidden layers,\n",
    "  ReLU nonlinearities, and a softmax loss function. This will also implement\n",
    "  dropout and batch normalization as options. For a network with L layers,\n",
    "  the architecture will be\n",
    "  \n",
    "  {affine - [batch norm] - relu - [dropout]} x (L - 1) - affine - softmax\n",
    "  \n",
    "  where batch normalization and dropout are optional, and the {...} block is\n",
    "  repeated L - 1 times.\n",
    "  \n",
    "  Similar to the TwoLayerNet above, learnable parameters are stored in the\n",
    "  self.params dictionary and will be learned using the Solver class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10,\n",
    "               dropout=0, use_batchnorm=False, reg=0.0,\n",
    "               weight_scale=1e-2, dtype=np.float32, seed=None):\n",
    "    \"\"\"\n",
    "    Initialize a new FullyConnectedNet.\n",
    "    \n",
    "    Inputs:\n",
    "    - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "    - input_dim: An integer giving the size of the input.\n",
    "    - num_classes: An integer giving the number of classes to classify.\n",
    "    - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=0 then\n",
    "      the network should not use dropout at all.\n",
    "    - use_batchnorm: Whether or not the network should use batch normalization.\n",
    "    - reg: Scalar giving L2 regularization strength.\n",
    "    - weight_scale: Scalar giving the standard deviation for random\n",
    "      initialization of the weights.\n",
    "    - dtype: A numpy datatype object; all computations will be performed using\n",
    "      this datatype. float32 is faster but less accurate, so you should use\n",
    "      float64 for numeric gradient checking.\n",
    "    - seed: If not None, then pass this random seed to the dropout layers. This\n",
    "      will make the dropout layers deteriminstic so we can gradient check the\n",
    "      model.\n",
    "    \"\"\"\n",
    "    self.use_batchnorm = use_batchnorm\n",
    "    self.use_dropout = dropout > 0\n",
    "    self.reg = reg\n",
    "    self.num_layers = 1 + len(hidden_dims)\n",
    "    self.dtype = dtype\n",
    "    self.params = {}\n",
    "    \n",
    "    self.hidden_dims = hidden_dims\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: Initialize the parameters of the network, storing all values in    #\n",
    "    # the self.params dictionary. Store weights and biases for the first layer #\n",
    "    # in W1 and b1; for the second layer use W2 and b2, etc. Weights should be #\n",
    "    # initialized from a normal distribution with standard deviation equal to  #\n",
    "    # weight_scale and biases should be initialized to zero.                   #\n",
    "    #                                                                          #\n",
    "    # When using batch normalization, store scale and shift parameters for the #\n",
    "    # first layer in gamma1 and beta1; for the second layer use gamma2 and     #\n",
    "    # beta2, etc. Scale parameters should be initialized to one and shift      #\n",
    "    # parameters should be initialized to zero.                                #\n",
    "    ############################################################################\n",
    "    # initialize hidden layers\n",
    "    for i in xrange(self.num_layers):\n",
    "        if i == 0:\n",
    "            self.params['W1'] = np.random.normal(scale=weight_scale, size=(input_dim, hidden_dims[i]))\n",
    "            self.params['b1'] = np.zeros(hidden_dims[i])\n",
    "        elif i == len(hidden_dims):\n",
    "            self.params['W'+str(i+1)] = np.random.normal(scale=weight_scale, size=(hidden_dims[i - 1], num_classes))\n",
    "            self.params['b'+str(i+1)] = np.zeros(num_classes)\n",
    "        else:\n",
    "            self.params['W'+str(i+1)] = np.random.normal(scale=weight_scale, size=(hidden_dims[i - 1], hidden_dims[i]))\n",
    "            self.params['b'+str(i+1)] = np.zeros(hidden_dims[i])\n",
    "        if self.use_batchnorm:\n",
    "            self.params['gamma'+str(i+1)] = np.ones_like(self.params['b'+str(i+1)])\n",
    "            self.params['beta'+str(i+1)] = np.zeros_like(self.params['b'+str(i+1)])\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    # When using dropout we need to pass a dropout_param dictionary to each\n",
    "    # dropout layer so that the layer knows the dropout probability and the mode\n",
    "    # (train / test). You can pass the same dropout_param to each dropout layer.\n",
    "    self.dropout_param = {}\n",
    "    if self.use_dropout:\n",
    "      self.dropout_param = {'mode': 'train', 'p': dropout}\n",
    "      if seed is not None:\n",
    "        self.dropout_param['seed'] = seed\n",
    "    \n",
    "    # With batch normalization we need to keep track of running means and\n",
    "    # variances, so we need to pass a special bn_param object to each batch\n",
    "    # normalization layer. You should pass self.bn_params[0] to the forward pass\n",
    "    # of the first batch normalization layer, self.bn_params[1] to the forward\n",
    "    # pass of the second batch normalization layer, etc.\n",
    "    self.bn_params = []\n",
    "    if self.use_batchnorm:\n",
    "      self.bn_params = [{'mode': 'train'} for i in xrange(self.num_layers - 1)]\n",
    "    \n",
    "    # Cast all parameters to the correct datatype\n",
    "    for k, v in self.params.iteritems():\n",
    "      self.params[k] = v.astype(dtype)\n",
    "\n",
    "    \n",
    "  def test(self, x):\n",
    "    x = x + 2\n",
    "    return x \n",
    "  def testp(self, x):\n",
    "    print self.test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "reg = 0\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64,\n",
    "                            use_batchnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "model.testp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "class FullyConnectedNet(object):   \n",
    "    def test(self, x):\n",
    "        x = x + 2\n",
    "        return x \n",
    "    def testp(self, x):\n",
    "        print self.test(x)\n",
    "model = FullyConnectedNet()\n",
    "model.testp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n",
      "[[-0.28052096  1.3913085   0.44913993  1.17697559 -1.79429114 -1.06634965\n",
      "  -1.03610116  0.66588288]\n",
      " [ 1.75579406 -0.45590674  0.79491511  1.02529582 -0.84549782 -0.01618443\n",
      "   0.93610738 -0.91668052]\n",
      " [ 1.59860916 -0.24172674  1.56332275  0.95894328 -1.40603464  0.20009142\n",
      "  -0.69338331 -0.62930264]]\n"
     ]
    }
   ],
   "source": [
    "y = np.random.randn(3,8)\n",
    "print y.shape\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "incompatible shape for a non-contiguous array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-658cf54ec6a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# res.shape = (F, N, out_h, out_w)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: incompatible shape for a non-contiguous array"
     ]
    }
   ],
   "source": [
    "# res.shape = (F, N, out_h, out_w)\n",
    "y.shape = (3, 2, 2, 2)\n",
    "print y.shape\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 2, 2)\n",
      "[[[[-0.28052096  1.3913085 ]\n",
      "   [ 0.44913993  1.17697559]]\n",
      "\n",
      "  [[-1.79429114 -1.06634965]\n",
      "   [-1.03610116  0.66588288]]]\n",
      "\n",
      "\n",
      " [[[ 1.75579406 -0.45590674]\n",
      "   [ 0.79491511  1.02529582]]\n",
      "\n",
      "  [[-0.84549782 -0.01618443]\n",
      "   [ 0.93610738 -0.91668052]]]\n",
      "\n",
      "\n",
      " [[[ 1.59860916 -0.24172674]\n",
      "   [ 1.56332275  0.95894328]]\n",
      "\n",
      "  [[-1.40603464  0.20009142]\n",
      "   [-0.69338331 -0.62930264]]]]\n"
     ]
    }
   ],
   "source": [
    "y = y.transpose(1, 0, 2, 3)\n",
    "print y.shape\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = y[0,:,:,:].reshape((y.shape[1], -1)).argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dx = np.zeros(y[0,:,:,:].reshape((y.shape[1], -1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'arrange'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-457412019930>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'arrange'"
     ]
    }
   ],
   "source": [
    "np.arrange(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dx[np.arange(tmp.shape[0]), tmp] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0,:,:,:].reshape((y.shape[1], -1)).argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -1.76052684e+00,  -8.02339704e-02,  -1.12930029e+00,\n",
       "           1.26964781e+00],\n",
       "        [  3.63978163e-01,  -6.66566902e-01,  -1.76190580e-01,\n",
       "           6.96327135e-01],\n",
       "        [  6.08082859e-01,   4.81725745e-01,  -3.10156566e-04,\n",
       "           6.28689210e-01]],\n",
       "\n",
       "       [[  2.63590653e-02,  -8.42704182e-01,  -1.21031368e+00,\n",
       "           4.97127314e-01],\n",
       "        [ -2.95835471e-01,  -9.35444372e-01,  -7.49337757e-01,\n",
       "           7.96631632e-01],\n",
       "        [  3.44873469e-01,   6.64484740e-01,  -2.35936399e+00,\n",
       "          -6.93551151e-01]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape(2, 3, -1, order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3], [3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [2, 2, 2]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 4],\n",
       "       [4, 4, 4],\n",
       "       [4, 4, 4]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot((x - y).T, (x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.var((x)[1, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "  The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "  examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "  reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "  then transform it to an output vector of dimension M.\n",
    "\n",
    "  Inputs:\n",
    "  - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "  - w: A numpy array of weights, of shape (D, M)\n",
    "  - b: A numpy array of biases, of shape (M,)\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - out: output, of shape (N, M)\n",
    "  - cache: (x, w, b)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "  # will need to reshape the input into rows.                                 #\n",
    "  #############################################################################\n",
    "  # flatten \n",
    "  z = np.reshape(x, (x.shape[0], -1)) # N X D\n",
    "  out = z.dot(w) + b\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, w, b)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for an affine layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivative, of shape (N, M)\n",
    "  - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "  - dw: Gradient with respect to w, of shape (D, M)\n",
    "  - db: Gradient with respect to b, of shape (M,)\n",
    "  \"\"\"\n",
    "  x, w, b = cache\n",
    "  dx, dw, db = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the affine backward pass.                                 #\n",
    "  #############################################################################\n",
    "  dx = np.dot(dout, w.T)\n",
    "  dx = np.reshape(dx, x.shape)\n",
    "  z = np.reshape(x, (x.shape[0], -1)) # N X D\n",
    "  dw = z.T.dot(dout)\n",
    "  db = dout.sum(0)\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx, dw, db\n",
    "\n",
    "\n",
    "def relu_forward(x):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "  Input:\n",
    "  - x: Inputs, of any shape\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output, of the same shape as x\n",
    "  - cache: x\n",
    "  \"\"\"\n",
    "  #out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the ReLU forward pass.                                    #\n",
    "  #############################################################################\n",
    "  out = np.maximum(0, x)\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = x\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "  Input:\n",
    "  - dout: Upstream derivatives, of any shape\n",
    "  - cache: Input x, of same shape as dout\n",
    "\n",
    "  Returns:\n",
    "  - dx: Gradient with respect to x\n",
    "  \"\"\"\n",
    "  dx, x = None, cache\n",
    "  #############################################################################\n",
    "  # TODO: Implement the ReLU backward pass.                                   #\n",
    "  #############################################################################\n",
    "  dx = dout\n",
    "  dx[x <= 0] = 0\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx\n",
    "\n",
    "\n",
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "  \"\"\"\n",
    "  Forward pass for batch normalization.\n",
    "  \n",
    "  During training the sample mean and (uncorrected) sample variance are\n",
    "  computed from minibatch statistics and used to normalize the incoming data.\n",
    "  During training we also keep an exponentially decaying running mean of the mean\n",
    "  and variance of each feature, and these averages are used to normalize data\n",
    "  at test-time.\n",
    "\n",
    "  At each timestep we update the running averages for mean and variance using\n",
    "  an exponential decay based on the momentum parameter:\n",
    "\n",
    "  running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "  running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "\n",
    "  Note that the batch normalization paper suggests a different test-time\n",
    "  behavior: they compute sample mean and variance for each feature using a\n",
    "  large number of training images rather than using a running average. For\n",
    "  this implementation we have chosen to use running averages instead since\n",
    "  they do not require an additional estimation step; the torch7 implementation\n",
    "  of batch normalization also uses running averages.\n",
    "\n",
    "  Input:\n",
    "  - x: Data of shape (N, D)\n",
    "  - gamma: Scale parameter of shape (D,)\n",
    "  - beta: Shift paremeter of shape (D,)\n",
    "  - bn_param: Dictionary with the following keys:\n",
    "    - mode: 'train' or 'test'; required\n",
    "    - eps: Constant for numeric stability\n",
    "    - momentum: Constant for running mean / variance.\n",
    "    - running_mean: Array of shape (D,) giving running mean of features\n",
    "    - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: of shape (N, D)\n",
    "  - cache: A tuple of values needed in the backward pass\n",
    "  \"\"\"\n",
    "  mode = bn_param['mode']\n",
    "  eps = bn_param.get('eps', 1e-5)\n",
    "  momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "  N, D = x.shape\n",
    "  running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "  running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "\n",
    "  out, cache = None, None\n",
    "  if mode == 'train':\n",
    "    #############################################################################\n",
    "    # TODO: Implement the training-time forward pass for batch normalization.   #\n",
    "    # Use minibatch statistics to compute the mean and variance, use these      #\n",
    "    # statistics to normalize the incoming data, and scale and shift the        #\n",
    "    # normalized data using gamma and beta.                                     #\n",
    "    #                                                                           #\n",
    "    # You should store the output in the variable out. Any intermediates that   #\n",
    "    # you need for the backward pass should be stored in the cache variable.    #\n",
    "    #                                                                           #\n",
    "    # You should also use your computed sample mean and variance together with  #\n",
    "    # the momentum variable to update the running mean and running variance,    #\n",
    "    # storing your result in the running_mean and running_var variables.        #\n",
    "    #############################################################################\n",
    "    pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "  elif mode == 'test':\n",
    "    #############################################################################\n",
    "    # TODO: Implement the test-time forward pass for batch normalization. Use   #\n",
    "    # the running mean and variance to normalize the incoming data, then scale  #\n",
    "    # and shift the normalized data using gamma and beta. Store the result in   #\n",
    "    # the out variable.                                                         #\n",
    "    #############################################################################\n",
    "    pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "  else:\n",
    "    raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "\n",
    "  # Store the updated running means back into bn_param\n",
    "  bn_param['running_mean'] = running_mean\n",
    "  bn_param['running_var'] = running_var\n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Backward pass for batch normalization.\n",
    "  \n",
    "  For this implementation, you should write out a computation graph for\n",
    "  batch normalization on paper and propagate gradients backward through\n",
    "  intermediate nodes.\n",
    "  \n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of shape (N, D)\n",
    "  - cache: Variable of intermediates from batchnorm_forward.\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "  - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "  - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "  \"\"\"\n",
    "  dx, dgamma, dbeta = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the backward pass for batch normalization. Store the      #\n",
    "  # results in the dx, dgamma, and dbeta variables.                           #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def batchnorm_backward_alt(dout, cache):\n",
    "  \"\"\"\n",
    "  Alternative backward pass for batch normalization.\n",
    "  \n",
    "  For this implementation you should work out the derivatives for the batch\n",
    "  normalizaton backward pass on paper and simplify as much as possible. You\n",
    "  should be able to derive a simple expression for the backward pass.\n",
    "  \n",
    "  Note: This implementation should expect to receive the same cache variable\n",
    "  as batchnorm_backward, but might not use all of the values in the cache.\n",
    "  \n",
    "  Inputs / outputs: Same as batchnorm_backward\n",
    "  \"\"\"\n",
    "  dx, dgamma, dbeta = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the backward pass for batch normalization. Store the      #\n",
    "  # results in the dx, dgamma, and dbeta variables.                           #\n",
    "  #                                                                           #\n",
    "  # After computing the gradient with respect to the centered inputs, you     #\n",
    "  # should be able to compute gradients with respect to the inputs in a       #\n",
    "  # single statement; our implementation fits on a single 80-character line.  #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  \n",
    "  return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def dropout_forward(x, dropout_param):\n",
    "  \"\"\"\n",
    "  Performs the forward pass for (inverted) dropout.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of any shape\n",
    "  - dropout_param: A dictionary with the following keys:\n",
    "    - p: Dropout parameter. We drop each neuron output with probability p.\n",
    "    - mode: 'test' or 'train'. If the mode is train, then perform dropout;\n",
    "      if the mode is test, then just return the input.\n",
    "    - seed: Seed for the random number generator. Passing seed makes this\n",
    "      function deterministic, which is needed for gradient checking but not in\n",
    "      real networks.\n",
    "\n",
    "  Outputs:\n",
    "  - out: Array of the same shape as x.\n",
    "  - cache: A tuple (dropout_param, mask). In training mode, mask is the dropout\n",
    "    mask that was used to multiply the input; in test mode, mask is None.\n",
    "  \"\"\"\n",
    "  p, mode = dropout_param['p'], dropout_param['mode']\n",
    "  if 'seed' in dropout_param:\n",
    "    np.random.seed(dropout_param['seed'])\n",
    "\n",
    "  mask = None\n",
    "  out = None\n",
    "\n",
    "  if mode == 'train':\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the training phase forward pass for inverted dropout.   #\n",
    "    # Store the dropout mask in the mask variable.                            #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                            END OF YOUR CODE                             #\n",
    "    ###########################################################################\n",
    "  elif mode == 'test':\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the test phase forward pass for inverted dropout.       #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                            END OF YOUR CODE                             #\n",
    "    ###########################################################################\n",
    "\n",
    "  cache = (dropout_param, mask)\n",
    "  out = out.astype(x.dtype, copy=False)\n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Perform the backward pass for (inverted) dropout.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of any shape\n",
    "  - cache: (dropout_param, mask) from dropout_forward.\n",
    "  \"\"\"\n",
    "  dropout_param, mask = cache\n",
    "  mode = dropout_param['mode']\n",
    "  \n",
    "  dx = None\n",
    "  if mode == 'train':\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the training phase backward pass for inverted dropout.  #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                            END OF YOUR CODE                             #\n",
    "    ###########################################################################\n",
    "  elif mode == 'test':\n",
    "    dx = dout\n",
    "  return dx\n",
    "\n",
    "\n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "  \"\"\"\n",
    "  A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "  The input consists of N data points, each with C channels, height H and width\n",
    "  W. We convolve each input with F different filters, where each filter spans\n",
    "  all C channels and has height HH and width HH.\n",
    "\n",
    "  Input:\n",
    "  - x: Input data of shape (N, C, H, W)\n",
    "  - w: Filter weights of shape (F, C, HH, WW)\n",
    "  - b: Biases, of shape (F,)\n",
    "  - conv_param: A dictionary with the following keys:\n",
    "    - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "      horizontal and vertical directions.\n",
    "    - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "    H' = 1 + (H + 2 * pad - HH) / stride\n",
    "    W' = 1 + (W + 2 * pad - WW) / stride\n",
    "  - cache: (x, w, b, conv_param)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the convolutional forward pass.                           #\n",
    "  # Hint: you can use the function np.pad for padding.                        #\n",
    "  #############################################################################\n",
    "  # construct X_col matrix, with padding for im2col operation\n",
    "  N, C, H, W = x.shape\n",
    "  F, C, HH, WW = w.shape\n",
    "  pad = conv_param['pad']\n",
    "  stride = conv_param['stride']\n",
    "  H_prime = 1 + (H + 2*pad - HH) / stride\n",
    "  W_prime = 1 + (W + 2*pad - WW) / stride\n",
    "  out = np.zeros((N, F, H_prime, W_prime))\n",
    "  # padding\n",
    "  x_padded = np.pad(x, ((0,0), (0, 0), (1, 1), (1, 1)), 'constant')\n",
    "  for im in xrange(N):\n",
    "    image = x_padded[im, :, :, :]\n",
    "    H_p = H + 2*pad\n",
    "    W_p = W + 2*pad   \n",
    "    # im2col\n",
    "    counter = 0\n",
    "    X_col = np.zeros((HH*WW*C, H_prime*W_prime))\n",
    "    for i in xrange(H_p):\n",
    "        # Fix y dimension, slice x dim, step size stride\n",
    "        end_i = WW + stride*i\n",
    "        if end_i <= H_p:\n",
    "            for j in xrange(W_p):\n",
    "                end_j = WW + stride*j\n",
    "                if end_j <= W_p:\n",
    "                    # stratch the window out\n",
    "                    X_col[:, counter] = image[:, (end_i-HH):end_i, (end_j - WW):end_j].ravel() # HH X WW X C\n",
    "                    counter += 1\n",
    "    #for f in xrange(F):\n",
    "    weight = w.reshape(F, -1) #[f, :, :, :].ravel()\n",
    "    out[im, :, :, :] = (np.dot(weight, X_col) + b[np.newaxis].T).reshape(F, H_prime, -1)\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, w, b, conv_param)\n",
    "  #cache = (X_col, w, b, conv_param)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "  \"\"\"\n",
    "  A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives.\n",
    "  - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x\n",
    "  - dw: Gradient with respect to w\n",
    "  - db: Gradient with respect to b\n",
    "  \"\"\"\n",
    "  dx, dw, db = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the convolutional backward pass.                          #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx, dw, db\n",
    "\n",
    "\n",
    "def max_pool_forward_naive(x, pool_param):\n",
    "  \"\"\"\n",
    "  A naive implementation of the forward pass for a max pooling layer.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C, H, W)\n",
    "  - pool_param: dictionary with the following keys:\n",
    "    - 'pool_height': The height of each pooling region\n",
    "    - 'pool_width': The width of each pooling region\n",
    "    - 'stride': The distance between adjacent pooling regions\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output data\n",
    "  - cache: (x, pool_param)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the max pooling forward pass                              #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, pool_param)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def max_pool_backward_naive(dout, cache):\n",
    "  \"\"\"\n",
    "  A naive implementation of the backward pass for a max pooling layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives\n",
    "  - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "\n",
    "  Returns:\n",
    "  - dx: Gradient with respect to x\n",
    "  \"\"\"\n",
    "  dx = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the max pooling backward pass                             #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx\n",
    "\n",
    "\n",
    "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for spatial batch normalization.\n",
    "  \n",
    "  Inputs:\n",
    "  - x: Input data of shape (N, C, H, W)\n",
    "  - gamma: Scale parameter, of shape (C,)\n",
    "  - beta: Shift parameter, of shape (C,)\n",
    "  - bn_param: Dictionary with the following keys:\n",
    "    - mode: 'train' or 'test'; required\n",
    "    - eps: Constant for numeric stability\n",
    "    - momentum: Constant for running mean / variance. momentum=0 means that\n",
    "      old information is discarded completely at every time step, while\n",
    "      momentum=1 means that new information is never incorporated. The\n",
    "      default of momentum=0.9 should work well in most situations.\n",
    "    - running_mean: Array of shape (D,) giving running mean of features\n",
    "    - running_var Array of shape (D,) giving running variance of features\n",
    "    \n",
    "  Returns a tuple of:\n",
    "  - out: Output data, of shape (N, C, H, W)\n",
    "  - cache: Values needed for the backward pass\n",
    "  \"\"\"\n",
    "  out, cache = None, None\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Implement the forward pass for spatial batch normalization.         #\n",
    "  #                                                                           #\n",
    "  # HINT: You can implement spatial batch normalization using the vanilla     #\n",
    "  # version of batch normalization defined above. Your implementation should  #\n",
    "  # be very short; ours is less than five lines.                              #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def spatial_batchnorm_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for spatial batch normalization.\n",
    "  \n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of shape (N, C, H, W)\n",
    "  - cache: Values from the forward pass\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
    "  - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
    "  - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
    "  \"\"\"\n",
    "  dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Implement the backward pass for spatial batch normalization.        #\n",
    "  #                                                                           #\n",
    "  # HINT: You can implement spatial batch normalization using the vanilla     #\n",
    "  # version of batch normalization defined above. Your implementation should  #\n",
    "  # be very short; ours is less than five lines.                              #\n",
    "  #############################################################################\n",
    "  pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  return dx, dgamma, dbeta\n",
    "  \n",
    "\n",
    "def svm_loss(x, y):\n",
    "  \"\"\"\n",
    "  Computes the loss and gradient using for multiclass SVM classification.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "  - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar giving the loss\n",
    "  - dx: Gradient of the loss with respect to x\n",
    "  \"\"\"\n",
    "  N = x.shape[0]\n",
    "  correct_class_scores = x[np.arange(N), y]\n",
    "  margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)\n",
    "  margins[np.arange(N), y] = 0\n",
    "  loss = np.sum(margins) / N\n",
    "  num_pos = np.sum(margins > 0, axis=1)\n",
    "  dx = np.zeros_like(x)\n",
    "  dx[margins > 0] = 1\n",
    "  dx[np.arange(N), y] -= num_pos\n",
    "  dx /= N\n",
    "  return loss, dx\n",
    "\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "  \"\"\"\n",
    "  Computes the loss and gradient for softmax classification.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "  - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar giving the loss\n",
    "  - dx: Gradient of the loss with respect to x\n",
    "  \"\"\"\n",
    "  probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "  probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "  N = x.shape[0]\n",
    "  loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
    "  dx = probs.copy()\n",
    "  dx[np.arange(N), y] -= 1\n",
    "  dx /= N\n",
    "  return loss, dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
